{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a51051d6"
      },
      "source": [
        "# 3.1 Anatomy of a Neural Network\n",
        "\n",
        "This notebook provides an overview of the fundamental components of a neural network.\n",
        "\n",
        "## 1. Layers: The Building Blocks of Deep Learning\n",
        "\n",
        "Neural networks are composed of layers. Each layer performs a transformation on the input data and passes it to the next layer.\n",
        "\n",
        "*   **Input Layer:** The first layer that receives the raw input data.\n",
        "*   **Hidden Layers:** Layers between the input and output layers. These layers perform complex computations on the data.\n",
        "*   **Output Layer:** The final layer that produces the network's output.\n",
        "\n",
        "### Types of Layers\n",
        "\n",
        "*   **Dense (Fully Connected) Layers:** Each neuron in a dense layer is connected to every neuron in the previous layer.\n",
        "    *   Equation: $y = \\sigma(Wx + b)$\n",
        "        *   $x$: Input vector\n",
        "        *   $W$: Weight matrix\n",
        "        *   $b$: Bias vector\n",
        "        *   $\\sigma$: Activation function\n",
        "*   **Convolutional Layers:** Primarily used for processing grid-like data such as images. They apply convolutional filters to the input.\n",
        "*   **Recurrent Layers:** Designed for sequential data, such as text or time series. They have internal memory to process sequences.\n",
        "\n",
        "### Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into the network, enabling it to learn complex patterns.\n",
        "\n",
        "*   **ReLU (Rectified Linear Unit):** $\\text{ReLU}(x) = \\max(0, x)$\n",
        "*   **Sigmoid:** $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
        "*   **Tanh (Hyperbolic Tangent):** $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
        "\n",
        "### Code Example: Dense Layer\n",
        "\n",
        "This code demonstrates a simple dense layer using TensorFlow/Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89d1cb22",
        "outputId": "c6f35348-2c59-4363-d078-bb4d6de1d0a3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Create a simple Dense layer\n",
        "dense_layer = Dense(units=10, activation='relu', input_shape=(5,))\n",
        "\n",
        "# Generate some dummy input data\n",
        "input_data = np.random.rand(1, 5) # Batch size of 1, 5 features\n",
        "\n",
        "# Pass the input data through the layer\n",
        "output_data = dense_layer(input_data)\n",
        "\n",
        "print(\"Input data shape:\", input_data.shape)\n",
        "print(\"Output data shape:\", output_data.shape)\n",
        "print(\"Output data:\\n\", output_data.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data shape: (1, 5)\n",
            "Output data shape: (1, 10)\n",
            "Output data:\n",
            " [[0.         0.38461566 0.         0.10109182 0.3938445  0.\n",
            "  0.         0.09975459 0.         0.15508273]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e83e6df6"
      },
      "source": [
        "## 2. Models: Networks of Layers\n",
        "\n",
        "A neural network model is formed by stacking different layers together in a specific architecture.\n",
        "\n",
        "*   **Sequential Models:** Layers are stacked in a linear fashion, where the output of one layer serves as the input to the next.\n",
        "*   **Functional API Models:** Allows for more flexible architectures with multiple inputs, multiple outputs, and shared layers.\n",
        "\n",
        "### Code Example: Sequential Model\n",
        "\n",
        "This code builds a simple sequential model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "d6a4f478",
        "outputId": "22323d3d-d268-452e-d510-385d6767160a"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential([\n",
        "    Dense(units=64, activation='relu', input_shape=(784,)), # Input layer (implied) and first hidden layer\n",
        "    Dense(units=64, activation='relu'), # Second hidden layer\n",
        "    Dense(units=10, activation='softmax') # Output layer for classification (e.g., 10 classes)\n",
        "])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m55,050\u001b[0m (215.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,050</span> (215.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m55,050\u001b[0m (215.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,050</span> (215.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c0fb1be"
      },
      "source": [
        "## 3. Loss Functions and Optimizers: Keys to Configuring the Learning Process\n",
        "\n",
        "These components are crucial for training a neural network.\n",
        "\n",
        "*   **Loss Function:** Measures the difference between the network's predictions and the actual target values. The goal of training is to minimize this loss.\n",
        "    *   **Mean Squared Error (MSE):** Used for regression tasks. $MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$\n",
        "    *   **Categorical Crossentropy:** Used for multi-class classification tasks.\n",
        "    *   **Binary Crossentropy:** Used for binary classification tasks.\n",
        "*   **Optimizer:** An algorithm that updates the network's weights and biases based on the calculated loss to minimize it.\n",
        "    *   **Stochastic Gradient Descent (SGD):** Updates weights in the direction opposite to the gradient of the loss function.\n",
        "    *   **Adam:** An adaptive optimization algorithm that uses estimates of first and second moments of the gradients.\n",
        "    *   **RMSprop:** Another adaptive learning rate optimization algorithm.\n",
        "\n",
        "### Code Example: Compiling a Model\n",
        "\n",
        "This code compiles the previously created sequential model with a loss function and an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcedc7b0",
        "outputId": "2eed373d-8481-49d3-b3df-58d8a7fcff1a"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled successfully!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1822cc6e"
      },
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "This notebook covered the fundamental components of a neural network: layers, models, loss functions, and optimizers. We explored different types of layers, activation functions, and model architectures, as well as common loss functions and optimizers used for training.\n",
        "\n",
        "Understanding these building blocks is essential for constructing and training effective neural networks for various machine learning tasks. This notebook provided a basic introduction, and further exploration into specific layer types, advanced model architectures, and hyperparameter tuning is recommended for deeper understanding and practical application."
      ]
    }
  ]
}